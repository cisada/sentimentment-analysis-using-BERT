{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWmMnEhn/2mlKktle/ehVO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cisada/sentimentment-analysis-using-BERT/blob/main/sentiment_analysis_(Twitter_dataset).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TWITTER SENTIMENT ANALYSIS"
      ],
      "metadata": {
        "id": "jENrfd5Keh0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs6BAXEldBU_"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 . DATA COLLECTION"
      ],
      "metadata": {
        "id": "tWB08YfvgY1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv',\n",
        "                 encoding = 'latin',header=None)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "pOCVyZMVoiOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. DATA Labelling"
      ],
      "metadata": {
        "id": "fcsrA-Z8gdM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns = ['sentiment','id','date','query','user','text']\n",
        "data.head()"
      ],
      "metadata": {
        "id": "TBGciIeVhC4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(['id','date','query','user'],axis=1,inplace=True)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "RgqYuXRzhC1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lab_to_sentiment = {0:\"Negative\", 4:\"Positive\"}\n",
        "\n",
        "def label_decoder(label):\n",
        "  return lab_to_sentiment[label]\n",
        "data['sentiment'] = data['sentiment'].apply(lambda x: label_decoder(x))\n",
        "data.head()"
      ],
      "metadata": {
        "id": "w93N6ICihD1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_count = data['sentiment'].value_counts()\n",
        "val_count"
      ],
      "metadata": {
        "id": "lQ3A0ncMhElT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.bar(val_count.index,val_count.values)\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title(\"Sentiment Data Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R_LrsoBChEgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.sample(n=10)"
      ],
      "metadata": {
        "id": "p7ctys-UqWF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stemmer = SnowballStemmer('english')\n",
        "test_cleaning_re = '@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+'"
      ],
      "metadata": {
        "id": "qAO9VRRfhFOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text,stem=False):\n",
        "  text = re.sub(test_cleaning_re,' ',str(text).lower()).strip()\n",
        "  tokens = []\n",
        "  for token in text.split():\n",
        "    if token not in stop_words:\n",
        "      if stem:\n",
        "        tokens.append(stemmer.stem(token))\n",
        "      else:\n",
        "        tokens.append(token)\n",
        "  return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "Al_y0VK8hFIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'] = data['text'].apply(lambda x: preprocess(x))\n",
        "data.head()"
      ],
      "metadata": {
        "id": "giM7cSugrxyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "wc = WordCloud(max_words=1000,width=1600,height=800).generate(\" \".join(data[data['sentiment']=='Positive']['text']))\n",
        "plt.imshow(wc,interpolation='bilinear')"
      ],
      "metadata": {
        "id": "koMFb3Qqr7Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,20))\n",
        "wc = WordCloud(max_words = 2000,width = 1600,height = 800).generate(\" \".join(data[data['sentiment']=='Negative']['text']))\n",
        "plt.imshow(wc,interpolation = 'bilinear')"
      ],
      "metadata": {
        "id": "AC3HPv3wsJoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. MODEL TRAINING"
      ],
      "metadata": {
        "id": "nRQ5eBwrgxR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_SIZE = 0.8\n",
        "MAX_NB_WORDS = 100000\n",
        "MAX_SEQUENCE_LENGTH = 30\n",
        "EMBEDDING_DIM = 200"
      ],
      "metadata": {
        "id": "yGmfFB8yhGpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = train_test_split(data, test_size=1 - TRAIN_SIZE, random_state=7)\n",
        "print(\"Train Data Size:\", len(train_data))\n",
        "print(\"Test Data Size\", len(test_data))"
      ],
      "metadata": {
        "id": "VxMFrw9wszCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head(10)"
      ],
      "metadata": {
        "id": "sBWnpm5ttq1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "dPDxgc4JhHSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data['text'])\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index)\n",
        "print(\"Vocabulary Size :\", vocab_size)"
      ],
      "metadata": {
        "id": "ePAt6yzGgmQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = pad_sequences(tokenizer.texts_to_sequences(train_data['text']),\n",
        "                        maxlen=MAX_SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(test_data['text']),\n",
        "                       maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print(\"Training Data Tensor Shape:\", x_train.shape)\n",
        "print(\"Testing Data Tensor Shape:\", x_test.shape)"
      ],
      "metadata": {
        "id": "DOJpy9pquazs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = train_data['sentiment'].unique().tolist()"
      ],
      "metadata": {
        "id": "5SXnsJVHurvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### label encoding\n"
      ],
      "metadata": {
        "id": "A7AWoeMwux0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(train_data.sentiment.tolist())\n",
        "\n",
        "y_train = encoder.transform(train_data.sentiment.tolist())\n",
        "y_test = encoder.transform(test_data.sentiment.tolist())\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "RdpL97c-uxe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading pretrained GloVe word embedded\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "AYIuU7nvuwMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GLOVE_EMB = 'glove.6B.200d.txt'\n",
        "embeddings_index = {}\n",
        "f = open(GLOVE_EMB)\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = value = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "metadata": {
        "id": "X6OrzTPYIIWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "f = open(GLOVE_EMB)\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = value = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' %len(embeddings_index))"
      ],
      "metadata": {
        "id": "GNW6aoqHvi-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size+1,EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "EqgJAU80vi64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding"
      ],
      "metadata": {
        "id": "5NfpI-E6xyz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "improved vocal_size+1 ==> vocal_size"
      ],
      "metadata": {
        "id": "L7hkOXADdFf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = tf.keras.layers.Embedding(vocab_size+1,\n",
        "                                          EMBEDDING_DIM,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                          trainable=False)"
      ],
      "metadata": {
        "id": "y9T-5RYYvi5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tuner\n"
      ],
      "metadata": {
        "id": "VuhvdSfMyML8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, BatchNormalization, Activation, Dropout, MaxPooling1D, Concatenate, Dense, GlobalMaxPooling1D, Softmax, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "3DHDO-M-wxjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "  inputs = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "  # Embedding layer using the pre-trained embedding matrix\n",
        "  embedding = embedding_layer(inputs)\n",
        "\n",
        "  #first convolutional path\n",
        "  conv1 = Conv1D(filters=hp.Int('conv1_filters', min_value=32, max_value=256, step=32),\n",
        "                 kernel_size = hp.Int('conv1_kernal_size', min_value=2, max_value=5),\n",
        "                 padding = 'valid',activation='relu')(embedding)\n",
        "  conv1 = BatchNormalization()(conv1)\n",
        "  conv1 = Dropout(hp.Float('conv1_dropout', min_value=0.2, max_value=0.5, step=0.1))(conv1)\n",
        "  conv1 = MaxPooling1D(pool_size=2)(conv1)\n",
        "\n",
        "  #Second convolutional layer\n",
        "  conv2 = Conv1D(filters=hp.Int('conv2_filters', min_value=32, max_value=256, step=32),\n",
        "                 kernel_size = hp.Int('conv2_kernal_size', min_value=2, max_value=5),\n",
        "                 padding = 'valid',activation='relu')(embedding)\n",
        "  conv2 = BatchNormalization()(conv2)\n",
        "  conv2 = Dropout(hp.Float('conv2_dropout', min_value=0.2, max_value=0.5, step=0.1))(conv2)\n",
        "  conv2 = MaxPooling1D(pool_size=2)(conv2)\n",
        "\n",
        "  #Concatenate convolutional outputs\n",
        "  concat = Concatenate(axis=-1)([conv1, conv2])\n",
        "  flatten = Flatten()(concat)\n",
        "\n",
        "  #Fully connected layer\n",
        "  fc_units = hp.Int('fc_units', min_value=128, max_value=512, step=64)\n",
        "  fc = Dense(units=fc_units, activation='relu')(flatten)\n",
        "\n",
        "  #softmax layer\n",
        "  num_classes = 1\n",
        "  output = Dense(units=num_classes, activation='sigmoid')(fc)\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "  #compile the model\n",
        "  optimizer = Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log'))\n",
        "  model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n"
      ],
      "metadata": {
        "id": "eaT3BYtqzgPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner wordcloud"
      ],
      "metadata": {
        "id": "M6ZLWthYV-r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the tuner\n",
        "tuner = RandomSearch(build_model,\n",
        "                     objective='val_accuracy',\n",
        "                     max_trials=5,\n",
        "                     directory='twitter_sentiment_analysis',\n",
        "                     project_name='twitter_sentiment_analysis')\n",
        "\n",
        "# Perform the hyperparameter search\n",
        "tuner.search(x_train, y_train,\n",
        "             epochs=5,\n",
        "             validation_split=0.2,\n",
        "             callbacks=[early_stopping])\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Build the model with the best hyperparameters\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Train the model\n",
        "history = best_model.fit(x_train, y_train,\n",
        "                         epochs=10,\n",
        "                         validation_data=(x_test, y_test),\n",
        "                         callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "id": "zgR_7YDiI7TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation"
      ],
      "metadata": {
        "id": "MX2bcwZsS5gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s, (at,al) = plt.subplots(2,1,figsize=(16,10))\n",
        "at.plot(history.history['accuracy'], c='b')\n",
        "at.plot(history.history['val_accuracy'], c='r')\n",
        "at.set_title('model accuracy')\n",
        "at.set_ylabel('accuracy')\n",
        "at.set_xlabel('epoch')\n",
        "at.legend(['train','validation'], loc='upper left')\n",
        "\n",
        "al.plot(history.history['loss'], c='m')\n",
        "al.plot(history.history['val_loss'], c='c')\n",
        "al.set_title('model loss')\n",
        "al.set_ylabel('loss')\n",
        "al.set_xlabel('epoch')\n",
        "al.legend(['train','validation'], loc='upper left')\n"
      ],
      "metadata": {
        "id": "ON6XDqZFS4m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sentiment(score):\n",
        "   return 'Positive' if score>0.5 else 'Negative'\n",
        "scores = best_model.predict(x_test, verbose=1, batch_size=8000)\n",
        "y_pred_1d = [decode_sentiment(score) for score in scores]"
      ],
      "metadata": {
        "id": "CryLo84HTPeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion matrix\n",
        "\n",
        "import itertools\n",
        "form sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, title = 'confusion matrix', cmap=plt.cm.Blues):\n",
        "  cm = cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
        "  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "  plt.title(title)\n",
        "  plt.colorbar()\n",
        "  ticks_marks = np.arange(len(classes))\n",
        "  plt.xticks(ticks_marks, classes, fontsize=13)\n",
        "  plt.yticks(ticks_marks, classes, fontsize=13)\n",
        "\n",
        "\n",
        "\n",
        "  fmt = '.2f'\n",
        "  thresh = cm.max()/2\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, format(cm[i, j], fmt),\n",
        "             horizontalalignment = 'center',\n",
        "             color = 'white' if cm[i, j] > thresh else 'black')\n",
        "  plt.ylabel('True label', font_size=20)\n",
        "  plt.xlabel('Predicted label', font_size=20)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "nYOBVJn8Thre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cf_matrix = confusion_matrix(test_data.sentiment.to_list(), y_pred_1d)\n",
        "plt.figure(figsize=(8,8))\n",
        "plot_confusion_matrix(cf_matrix, classes=test_data.sentiment.unique(), title = 'confusion matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lAyEnP3HVPa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classification report\n",
        "print(\"Classification repoert\")\n",
        "print(classification_report(test_data.sentiment.to_list(), y_pred_1d))"
      ],
      "metadata": {
        "id": "kgHXoCppVqfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ROC Curve\n",
        "y_true_binary = np.where(np.array(test_data.sentiment.to_list()) == 'Positive', 1, 0)\n",
        "fpr, tpr, _ = roc_curve(y_true_binary, scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0,1)], [0,1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.xlabel('False Positive Rate', font_size=20)\n",
        "plt.ylabel('True Positive Rate', font_size=20)\n",
        "plt.title('Receiver Operating Characteristic', font_size=20)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DAss_9SCf6gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PR Curve\n",
        "precision, recall, _ = precision_recall_curve(y_true_binary, scores)\n",
        "average_precision = average_precision_score(y_true_binary, scores)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.step(recall, precision, color='darkblue', lw=2, label='PR Curve(area - %0.2f)' % average_precision)\n",
        "plt.fill_between(recall, precision, step='post', alpha=0.2, color='darkblue')\n",
        "plt.xlabel('Recall', font_size=20)\n",
        "plt.ylabel('Precision', font_size=20)\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('Precision-Recall curve')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mJAtfXACgXqv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}